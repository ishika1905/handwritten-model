{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zOJ2h_gIffx",
        "outputId": "c8310b7a-fb5c-4edd-dd20-b746df48dbb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "metadata": {
        "id": "azEWEv8pI9uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the training set, add data augmentation or normalization here\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alcI3yDfJCre",
        "outputId": "6a94d015-e6b1-437a-8341-4e899bf7ed41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 11416895.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 347766.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3196827.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3882316.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # Output: (32, 26, 26)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)  # Output: (64, 24, 24)\n",
        "        self.fc1 = nn.Linear(64 * 5 * 5, 128)  # Adjust the input features here\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2)  # Output: (32, 13, 13)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)  # Output: (64, 6, 6)\n",
        "        x = x.view(-1, 64 * 5 * 5)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "k8asTAgOJfI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load the training and test datasets\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "8tSeZav6KKh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
        "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Train the model for several epochs\n",
        "for epoch in range(1, 11):\n",
        "    train(model, device, train_loader, optimizer, criterion, epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL4Uw56aKMOp",
        "outputId": "593c709f-a742-4709-d4bb-7eaf4a0b98aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.331074\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.204851\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.098228\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.149701\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.086318\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.163080\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.037712\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.014779\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.009493\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.008883\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.048643\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.030548\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.083290\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.013666\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.043076\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.049077\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.021826\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.057680\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.022303\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089443\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.024963\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.045336\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.020821\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.007476\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.023689\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.007725\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.010178\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.009716\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.001632\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.003694\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.028308\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.003427\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.076125\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.005930\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.011080\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.015432\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.000180\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.003887\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.045934\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.031336\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003957\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.010000\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.004698\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.000096\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.022988\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.003596\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.041693\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005794\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.000996\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.003973\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032628\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.002843\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.126000\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.015829\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001420\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.009527\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.003606\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.003425\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.019638\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.006658\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.003578\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.048797\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.018784\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.002122\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.058776\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.002393\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.001158\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004508\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.026214\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.037645\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.000729\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.001273\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.001211\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001028\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.001034\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.001934\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.021997\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.007120\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.000374\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.017413\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.022340\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.002874\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000624\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000715\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001283\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.016450\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000585\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.000964\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.001212\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000866\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001122\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001019\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.009889\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000226\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.035348\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001550\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.000045\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000031\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.002989\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.001742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
        "\n",
        "# Evaluate the model\n",
        "test(model, device, test_loader, criterion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrgosFOvNtrp",
        "outputId": "fa46b177-4abf-4061-8dcf-8a2e4a255127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0023, Accuracy: 404/10000 (4.04%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"mnist_cnn.pth\")\n"
      ],
      "metadata": {
        "id": "x5m_x3f3N1vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cTSHXYjWXlU",
        "outputId": "bec011b1-d0d8-4b7e-fe1d-9d2bdbc9b17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path in Google Drive\n",
        "model_save_path = \"/content/drive/My Drive/mnist_cnn.pth\"\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), model_save_path)\n"
      ],
      "metadata": {
        "id": "E01NdjJ0a_Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/My\\ Drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FegZuKF_bAwf",
        "outputId": "ee450c17-be42-481d-e40b-6f9e378a87f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 17.jpg   badge.jpg\t     error.jpg\t\t  mnist_cnn.pth\n",
            " 2.jpg\t  Classroom\t    'ishika resume.pdf'  'Project proposal.gdoc'\n",
            " 7.jpg\t 'Colab Notebooks'   IshikaResume.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/mnist_cnn.pth\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWRY2EOUWbeG",
        "outputId": "467fcc73-567b-43c5-e5ca-ebd9d0d75ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.Resize((28, 28)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "    image = transform(image)\n",
        "    image = image.unsqueeze(0)  # Add batch dimension\n",
        "    return image"
      ],
      "metadata": {
        "id": "XcdjwDFJbbLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Path to the image in Google Drive\n",
        "image_path = \"/content/drive/My Drive/2.jpg\"  # Update this path if necessary\n",
        "input_image = preprocess_image(image_path)\n",
        "\n",
        "# Move input to the same device as the model\n",
        "input_image = input_image.to(device)\n",
        "\n",
        "# Get the prediction\n",
        "with torch.no_grad():\n",
        "    output = model(input_image)\n",
        "    predicted_class = output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "print(f\"Predicted class: {predicted_class.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vB-eehylbKDB",
        "outputId": "2dc469b4-2fbe-43a9-c274-979099319591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d3da445b4178>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Path to the image in Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/My Drive/2.jpg\"\u001b[0m  \u001b[0;31m# Update this path if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Move input to the same device as the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these print statements before the prediction\n",
        "print(\"Input image tensor shape:\", input_image.shape)\n",
        "print(\"Input image tensor values:\", input_image)\n",
        "\n",
        "# Get the prediction\n",
        "with torch.no_grad():\n",
        "    output = model(input_image)\n",
        "    print(\"Raw prediction scores:\", output)\n",
        "    predicted_class = output.argmax(dim=1, keepdim=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgMzb2a4dscH",
        "outputId": "b98fe211-8504-4bc7-fa06-3f5ccf652d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input image tensor shape: torch.Size([1, 1, 28, 28])\n",
            "Input image tensor values: tensor([[[[ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8088,  2.8088,  2.7960,  2.8215,  2.8215,  2.8215,  2.8088,\n",
            "            2.8088,  2.7960,  2.7960,  2.7960,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.6942,  2.4651,\n",
            "            2.1214,  1.3450,  0.9250,  1.0650,  1.1414,  1.0904,  1.0523,\n",
            "            0.9504,  0.8232,  1.0268,  0.8741,  1.2432,  2.0323,  2.7324,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.2996,  0.5686,  0.0849,\n",
            "           -0.1696, -0.3733, -0.4115, -0.4115, -0.4115, -0.4115, -0.3988,\n",
            "           -0.3988, -0.4115, -0.3988, -0.4115, -0.3988, -0.1569,  1.3450,\n",
            "            2.7451,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.0705, -0.2206, -0.4115,\n",
            "           -0.4115, -0.4115, -0.4115, -0.2715,  0.6322,  1.0777,  1.0650,\n",
            "            0.5177, -0.3606, -0.3988, -0.3733, -0.4115, -0.4115,  0.5559,\n",
            "            2.5160,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.3378, -0.0424, -0.4115,\n",
            "           -0.4115, -0.4115, -0.4115, -0.1442,  2.0323,  2.7960,  2.8215,\n",
            "            2.3633,  0.0467, -0.3733, -0.3224, -0.4115, -0.4115,  0.4413,\n",
            "            2.4142,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.3124, -0.0424, -0.4115,\n",
            "           -0.4115, -0.4115, -0.4115,  0.2377,  2.3887,  2.8088,  2.8215,\n",
            "            2.5669,  0.2377, -0.4115, -0.4115, -0.4115, -0.4115,  0.2377,\n",
            "            2.4142,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.7324,  1.1159, -0.3224,\n",
            "           -0.2842, -0.3097,  0.6195,  1.7650,  2.7706,  2.8088,  2.8215,\n",
            "            2.5924,  0.2758, -0.4115, -0.4115, -0.4115, -0.3988,  0.5940,\n",
            "            2.4524,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.4142,  1.4468,\n",
            "            0.5686,  1.1795,  2.6178,  2.8088,  2.8215,  2.8215,  2.8215,\n",
            "            2.6306,  0.6195, -0.3988, -0.4115, -0.4115, -0.2969,  0.7086,\n",
            "            2.6178,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8088,  2.7960,\n",
            "            2.6560,  2.7706,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.7069,  0.9759, -0.3860, -0.3988, -0.3988,  0.4286,  1.0523,\n",
            "            2.6051,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.7069,  0.9250, -0.1060, -0.2715, -0.3860,  0.8104,  1.2814,\n",
            "            2.5924,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.6178,  1.1541,  1.0395, -0.3097, -0.3478,  0.7213,  0.7722,\n",
            "            2.6178,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.7069,  2.0196,  0.6831, -0.3606, -0.3988,  0.3140,  1.0904,\n",
            "            2.6687,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.5797,  1.3705,  0.5049, -0.3988, -0.2460,  1.0141,  1.6123,\n",
            "            2.6942,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.7833,\n",
            "            1.9814,  1.0013,  0.1358, -0.4115,  0.1104,  0.9504,  1.5741,\n",
            "            2.7578,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8088,  2.6942,  2.4142,  1.9942,  1.4850,  0.8741,\n",
            "            0.0849, -0.1442, -0.3351, -0.2969, -0.1060,  0.9504,  2.1214,\n",
            "            2.7960,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.7960,  2.6815,\n",
            "            2.5542,  1.7269,  0.5559,  0.0213, -0.2715, -0.3860, -0.4115,\n",
            "           -0.3733, -0.2206, -0.0806,  0.3649,  0.7086,  1.4596,  2.5797,\n",
            "            2.8088,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8088,  2.7578,  2.5160,  2.1087,\n",
            "            1.2050, -0.1315, -0.4115, -0.4115, -0.3860, -0.3478, -0.0806,\n",
            "            0.1867,  1.0141,  1.8032,  2.3505,  2.2996,  2.6306,  2.7960,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8088,  2.6178,  1.7905,  1.1414,  0.2504,\n",
            "           -0.1824, -0.3988, -0.3733,  0.2122,  0.8104,  1.3959,  2.0451,\n",
            "            2.3124,  2.5797,  2.7324,  2.8088,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.7578,  2.1087,  1.8287,  0.6450, -0.3988,\n",
            "           -0.4115, -0.4115,  0.1613,  2.1978,  2.4778,  2.4396,  2.4269,\n",
            "            2.4778,  2.5797,  2.4396,  2.3378,  2.5160,  2.5797,  2.6051,\n",
            "            2.7833,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.7324,  1.8414,  1.7650,  0.1486, -0.4115,\n",
            "           -0.4115, -0.4115, -0.1442,  0.5177,  0.2631,  0.0467,  0.0213,\n",
            "            0.1358,  0.3777,  0.3013,  0.2758,  0.3395,  0.3395,  0.3013,\n",
            "            0.9123,  2.2742,  2.8088,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.7578,  1.8923,  1.4723, -0.0551, -0.4115,\n",
            "           -0.4115, -0.3988, -0.3733, -0.3478, -0.3733, -0.3478, -0.1951,\n",
            "           -0.1824, -0.2460, -0.3097, -0.3478, -0.4115, -0.3988, -0.3733,\n",
            "           -0.2460,  1.0650,  2.6433,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.7833,  1.6123,  0.4922, -0.2206, -0.2842,\n",
            "           -0.1569,  0.2377,  0.7722,  1.2050,  1.4087,  1.7269,  2.1214,\n",
            "            2.1596,  2.0323,  1.7269,  1.1414,  0.8868,  0.8995,  0.8232,\n",
            "            1.5232,  2.2869,  2.7578,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.5415,  1.5996,  1.4468,  1.5868,\n",
            "            1.9305,  2.5033,  2.7451,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.7960,  2.7578,  2.7451,  2.6051,\n",
            "            2.7706,  2.8088,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.8215,  2.8215,  2.8215,  2.8215,  2.7706,  2.8088,  2.8088,\n",
            "            2.8088,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,\n",
            "            2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215,  2.8215],\n",
            "          [ 2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,\n",
            "            2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,\n",
            "            2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,\n",
            "            2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687,  2.6687],\n",
            "          [ 0.4159,  0.9377,  0.9886,  0.5431,  0.3522,  0.3522,  0.3522,\n",
            "            0.3522,  0.3522,  0.3522,  0.3522,  0.3522,  0.3522,  0.3522,\n",
            "            0.3522,  0.3522,  0.3522,  0.3522,  0.3522,  0.3522,  0.3522,\n",
            "            0.3522,  0.3522,  0.3649,  0.4922,  0.5304,  0.5177,  0.3777],\n",
            "          [-0.3097,  0.4286,  0.5940, -0.0169, -0.4115, -0.4115, -0.4115,\n",
            "           -0.4115, -0.4115, -0.4115, -0.4115, -0.4115, -0.4115, -0.4115,\n",
            "           -0.4115, -0.4115, -0.4115, -0.4115, -0.4115, -0.4115, -0.4115,\n",
            "           -0.4115, -0.4115, -0.3733, -0.1060, -0.0551, -0.1060, -0.3733]]]])\n",
            "Raw prediction scores: tensor([[ 0.1237,  0.1875, -0.0684,  0.1296,  0.0811, -0.0660,  0.0129,  0.0411,\n",
            "          0.0726,  0.0645]])\n"
          ]
        }
      ]
    }
  ]
}